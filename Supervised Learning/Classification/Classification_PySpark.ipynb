{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classfication in PySpark MLib- Experimentation\n",
    "\n",
    "### Classification\n",
    "Classfication is a supervised Machine learning task where we want to automatically categorize the data into some pre-defined categorization method.\n",
    "Examples of classification might include sorting objects like flowers into various species or automatically labeling images into groups like cats, dogs fish, etc.\n",
    "To be able to do this though, we need to have traning data and a pre-defined dependent variable which is the column in your dataset that defines the categoeries you want to predict. \n",
    "\n",
    "Algorithms Available\n",
    "\n",
    "Pyspark offers the following algrothims for classification\n",
    "1. Logistic regression\n",
    "2. Naive Bayes\n",
    "3. One vs Rest\n",
    "4. Linear Support Vector Machine (SVC)\n",
    "5. Random forest classifier\n",
    "6. Gradient Boosted Tree (GBT) Classifier\n",
    "7.  Decision Tree Classifier\n",
    "8. Multilayer Perceptron Classifier (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 cores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gw02.itversity.com:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.2.6.5.0-292</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Classification</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7473b9c124e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PySpark instance\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Classification\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"cores\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Name: Autistic Spectrum Disorder Screening Data for Adult\n",
    "Autistic Spectrum Disorder (ASD) is a neurodevelopment condition associated with significant healthcare costs, and early diagnosis can significantly reduce these. Unfortunately, waiting times for an ASD diagnosis are lengthy and procedures are not cost effective. The economic impact of autism and the increase in the number of ASD cases across the world reveals an urgent need for the development of easily implemented and effective screening methods. Therefore, a time-efficient and accessible ASD screening is imminent to help health professionals and inform individuals whether they should pursue formal clinical diagnosis. The rapid growth in the number of ASD cases worldwide necessitates datasets related to behaviour traits. However, such datasets are rare making it difficult to perform thorough analyses to improve the efficiency, sensitivity, specificity and predictive accuracy of the ASD screening process. Presently, very limited autism datasets associated with clinical or screening are available and most of them are genetic in nature. Hence, we propose a new dataset related to autism screening of adults that contained 20 features to be utilised for further analysis especially in determining influential autistic traits and improving the classification of ASD cases. In this dataset, we record ten behavioural features (AQ-10-Adult) plus ten individuals characteristics that have proved to be effective in detecting the ASD cases from controls in behaviour science.\n",
    "\n",
    "### Source: \n",
    "https://www.kaggle.com/faizunnabi/autism-screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/user/harishmohan/Datasets/'\n",
    "As_df = spark.read.csv(path+'Autismscreening.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_Score</th>\n",
       "      <th>A2_Score</th>\n",
       "      <th>A3_Score</th>\n",
       "      <th>A4_Score</th>\n",
       "      <th>A5_Score</th>\n",
       "      <th>A6_Score</th>\n",
       "      <th>A7_Score</th>\n",
       "      <th>A8_Score</th>\n",
       "      <th>A9_Score</th>\n",
       "      <th>A10_Score</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jundice</th>\n",
       "      <th>austim</th>\n",
       "      <th>contry_of_res</th>\n",
       "      <th>used_app_before</th>\n",
       "      <th>result</th>\n",
       "      <th>age_desc</th>\n",
       "      <th>relation</th>\n",
       "      <th>Class/ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>Spain</td>\n",
       "      <td>no</td>\n",
       "      <td>8</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Parent</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>?</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Others</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>9</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1_Score  A2_Score  A3_Score  A4_Score  A5_Score  A6_Score  A7_Score  \\\n",
       "0         1         1         1         1         0         0         1   \n",
       "1         1         1         0         1         0         0         0   \n",
       "2         1         1         0         1         1         0         1   \n",
       "3         1         1         0         1         0         0         1   \n",
       "4         1         0         0         0         0         0         0   \n",
       "5         1         1         1         1         1         0         1   \n",
       "\n",
       "   A8_Score  A9_Score  A10_Score  ... gender       ethnicity jundice austim  \\\n",
       "0         1         0          0  ...      f  White-European      no     no   \n",
       "1         1         0          1  ...      m          Latino      no    yes   \n",
       "2         1         1          1  ...      m          Latino     yes    yes   \n",
       "3         1         0          1  ...      f  White-European      no    yes   \n",
       "4         1         0          0  ...      f               ?      no     no   \n",
       "5         1         1          1  ...      m          Others     yes     no   \n",
       "\n",
       "     contry_of_res used_app_before result       age_desc relation Class/ASD  \n",
       "0  'United States'              no      6  '18 and more'     Self        NO  \n",
       "1           Brazil              no      5  '18 and more'     Self        NO  \n",
       "2            Spain              no      8  '18 and more'   Parent       YES  \n",
       "3  'United States'              no      6  '18 and more'     Self        NO  \n",
       "4            Egypt              no      2  '18 and more'        ?        NO  \n",
       "5  'United States'              no      9  '18 and more'     Self       YES  \n",
       "\n",
       "[6 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "As_df.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A1_Score: integer (nullable = true)\n",
      " |-- A2_Score: integer (nullable = true)\n",
      " |-- A3_Score: integer (nullable = true)\n",
      " |-- A4_Score: integer (nullable = true)\n",
      " |-- A5_Score: integer (nullable = true)\n",
      " |-- A6_Score: integer (nullable = true)\n",
      " |-- A7_Score: integer (nullable = true)\n",
      " |-- A8_Score: integer (nullable = true)\n",
      " |-- A9_Score: integer (nullable = true)\n",
      " |-- A10_Score: integer (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ethnicity: string (nullable = true)\n",
      " |-- jundice: string (nullable = true)\n",
      " |-- austim: string (nullable = true)\n",
      " |-- contry_of_res: string (nullable = true)\n",
      " |-- used_app_before: string (nullable = true)\n",
      " |-- result: integer (nullable = true)\n",
      " |-- age_desc: string (nullable = true)\n",
      " |-- relation: string (nullable = true)\n",
      " |-- Class/ASD: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(As_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Class/ASD|count|\n",
      "+---------+-----+\n",
      "|      YES|  189|\n",
      "|       NO|  515|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "As_df.groupBy(\"Class/ASD\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "import math\n",
    "\n",
    "major_df = As_df.filter(col(\"Class/ASD\") == 'NO')\n",
    "minor_df = As_df.filter(col(\"Class/ASD\") == 'YES')\n",
    "ratio = math.ceil(major_df.count()/minor_df.count())\n",
    "print(\"ratio: {}\".format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Oversampling***: The idea of oversampling, is to duplicate the samples from under represented class, to inflate the number till it reaches the same level as the dominant class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Class/ASD|count|\n",
      "+---------+-----+\n",
      "|      YES|  567|\n",
      "|       NO|  515|\n",
      "+---------+-----+\n",
      "\n",
      "None\n",
      "1082\n"
     ]
    }
   ],
   "source": [
    "a = range(ratio)\n",
    "\n",
    "# duplicate the majority rows\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "\n",
    "# Combine both oversampled minority rows and previous majority rows\n",
    "combined_df = major_df.unionAll(oversampled_df)\n",
    "print(combined_df.groupBy('Class/ASD').count().show())\n",
    "print(combined_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_Score</th>\n",
       "      <th>A2_Score</th>\n",
       "      <th>A3_Score</th>\n",
       "      <th>A4_Score</th>\n",
       "      <th>A5_Score</th>\n",
       "      <th>A6_Score</th>\n",
       "      <th>A7_Score</th>\n",
       "      <th>A8_Score</th>\n",
       "      <th>A9_Score</th>\n",
       "      <th>A10_Score</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jundice</th>\n",
       "      <th>austim</th>\n",
       "      <th>contry_of_res</th>\n",
       "      <th>used_app_before</th>\n",
       "      <th>result</th>\n",
       "      <th>age_desc</th>\n",
       "      <th>relation</th>\n",
       "      <th>Class/ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>?</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1_Score  A2_Score  A3_Score  A4_Score  A5_Score  A6_Score  A7_Score  \\\n",
       "0         1         1         1         1         0         0         1   \n",
       "1         1         1         0         1         0         0         0   \n",
       "2         1         1         0         1         0         0         1   \n",
       "3         1         0         0         0         0         0         0   \n",
       "\n",
       "   A8_Score  A9_Score  A10_Score  ... gender       ethnicity jundice austim  \\\n",
       "0         1         0          0  ...      f  White-European      no     no   \n",
       "1         1         0          1  ...      m          Latino      no    yes   \n",
       "2         1         0          1  ...      f  White-European      no    yes   \n",
       "3         1         0          0  ...      f               ?      no     no   \n",
       "\n",
       "     contry_of_res used_app_before result       age_desc relation Class/ASD  \n",
       "0  'United States'              no      6  '18 and more'     Self        NO  \n",
       "1           Brazil              no      5  '18 and more'     Self        NO  \n",
       "2  'United States'              no      6  '18 and more'     Self        NO  \n",
       "3            Egypt              no      2  '18 and more'        ?        NO  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Data\n",
    "input_columns = combined_df.columns\n",
    "input_columns = input_columns[1:-1]\n",
    "\n",
    "dependent_var = 'Class/ASD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change label (class variable) to string type to prep for reindexing\n",
    "# Pyspark is expecting a zero indexed integer for the label column. \n",
    "# Just in case our data is not in that format... we will treat it by using the StringIndexer built in method\n",
    "renamed = combined_df.withColumn(\"label_str\", combined_df[dependent_var].cast(StringType())) #Rename and change to string type\n",
    "indexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\") #Pyspark is expecting the this naming convention \n",
    "indexed = indexer.fit(renamed).transform(renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_Score</th>\n",
       "      <th>A2_Score</th>\n",
       "      <th>A3_Score</th>\n",
       "      <th>A4_Score</th>\n",
       "      <th>A5_Score</th>\n",
       "      <th>A6_Score</th>\n",
       "      <th>A7_Score</th>\n",
       "      <th>A8_Score</th>\n",
       "      <th>A9_Score</th>\n",
       "      <th>A10_Score</th>\n",
       "      <th>...</th>\n",
       "      <th>jundice</th>\n",
       "      <th>austim</th>\n",
       "      <th>contry_of_res</th>\n",
       "      <th>used_app_before</th>\n",
       "      <th>result</th>\n",
       "      <th>age_desc</th>\n",
       "      <th>relation</th>\n",
       "      <th>Class/ASD</th>\n",
       "      <th>label_str</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>?</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1_Score  A2_Score  A3_Score  A4_Score  A5_Score  A6_Score  A7_Score  \\\n",
       "0         1         1         1         1         0         0         1   \n",
       "1         1         1         0         1         0         0         0   \n",
       "2         1         1         0         1         0         0         1   \n",
       "3         1         0         0         0         0         0         0   \n",
       "4         0         1         0         0         0         0         0   \n",
       "\n",
       "   A8_Score  A9_Score  A10_Score  ... jundice austim    contry_of_res  \\\n",
       "0         1         0          0  ...      no     no  'United States'   \n",
       "1         1         0          1  ...      no    yes           Brazil   \n",
       "2         1         0          1  ...      no    yes  'United States'   \n",
       "3         1         0          0  ...      no     no            Egypt   \n",
       "4         1         0          0  ...      no     no  'United States'   \n",
       "\n",
       "  used_app_before result       age_desc relation  Class/ASD label_str label  \n",
       "0              no      6  '18 and more'     Self         NO        NO   1.0  \n",
       "1              no      5  '18 and more'     Self         NO        NO   1.0  \n",
       "2              no      6  '18 and more'     Self         NO        NO   1.0  \n",
       "3              no      2  '18 and more'        ?         NO        NO   1.0  \n",
       "4              no      2  '18 and more'     Self         NO        NO   1.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string type data into the input column list to numberic\n",
    "numeric_inputs = []\n",
    "string_inputs = []\n",
    "for column in input_columns:\n",
    "    if str(indexed.schema[column].dataType) == 'StringType':\n",
    "        indexer = StringIndexer(inputCol=column, outputCol = column+ '_num')\n",
    "        indexed = indexer.fit(indexed).transform(indexed)\n",
    "        new_col_name = column+\"_num\"\n",
    "        string_inputs.append(new_col_name)\n",
    "    else:\n",
    "        numeric_inputs.append(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A1_Score: integer (nullable = true)\n",
      " |-- A2_Score: integer (nullable = true)\n",
      " |-- A3_Score: integer (nullable = true)\n",
      " |-- A4_Score: integer (nullable = true)\n",
      " |-- A5_Score: integer (nullable = true)\n",
      " |-- A6_Score: integer (nullable = true)\n",
      " |-- A7_Score: integer (nullable = true)\n",
      " |-- A8_Score: integer (nullable = true)\n",
      " |-- A9_Score: integer (nullable = true)\n",
      " |-- A10_Score: integer (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ethnicity: string (nullable = true)\n",
      " |-- jundice: string (nullable = true)\n",
      " |-- austim: string (nullable = true)\n",
      " |-- contry_of_res: string (nullable = true)\n",
      " |-- used_app_before: string (nullable = true)\n",
      " |-- result: integer (nullable = true)\n",
      " |-- age_desc: string (nullable = true)\n",
      " |-- relation: string (nullable = true)\n",
      " |-- Class/ASD: string (nullable = true)\n",
      " |-- label_str: string (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- age_num: double (nullable = false)\n",
      " |-- gender_num: double (nullable = false)\n",
      " |-- ethnicity_num: double (nullable = false)\n",
      " |-- jundice_num: double (nullable = false)\n",
      " |-- austim_num: double (nullable = false)\n",
      " |-- contry_of_res_num: double (nullable = false)\n",
      " |-- used_app_before_num: double (nullable = false)\n",
      " |-- age_desc_num: double (nullable = false)\n",
      " |-- relation_num: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating for Skewness and outliers\n",
    "Skewness meausres how much a distribution of values deviates from symmetry aroud the mean. A value of zero mean the distribution is symmetric, while a postive skweness indicated a greater number of small values, and a negative value indicates a greater number of lager value. \\\n",
    "As a general rule of thumb: \n",
    "    - If skewness is less than -1 or greater than 1, the distribution is highly skewed\n",
    "    - If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed\n",
    "    - If skewness is between -0.5 and 0.5 the distribution is approximately symmetric\n",
    "    \n",
    "A common recommendation for treating skweness is either a log transformation for postive skewness data or an exponential transformation for negative skewed data\n",
    "Outliers: One common way to correct outliers is by flooring and capping which means editing ant value that is above or below a certain threshold (99th percentile or 1st percentile) back to the highest/lowest value in that percentile. For example, if the 99th percentile is 96 and there is a value of 1,000, you would change that value to 96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat for Skewness\n",
    "# Flooring and capping\n",
    "# if right skew take the log+1\n",
    "# if left skew do exp transformation\n",
    "\n",
    "# Create empty dictionary d\n",
    "d = {}\n",
    "# Create a dictionary of quantiles from your numberic cols\n",
    "# I'm doing the top and bottom 1% but you can adjust if needed\n",
    "\n",
    "for col in numeric_inputs:\n",
    "    d[col] = indexed.approxQuantile(col, [0.01, 0.99], 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_inputs: \n",
    "    skew = indexed.agg(skewness(indexed[col])).collect()\n",
    "    skew = skew[0][0]\n",
    "    # If skewness is found\n",
    "    # This function will make the appropriate corrections\n",
    "    \n",
    "    if skew > 1: #If right shew, floor, cap and log(x+1)\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        log(when(combined_df[col] < d[col][0], d[col][0]) \\\n",
    "        .when(indexed[col] > d[col][1], d[col][1]) \\\n",
    "        .otherwise(indexed[col])+ 1).alias(col))\n",
    "        print(col+ \" has been treated for postive (right) skewness. (skew=)\", skew, \")\")\n",
    "        \n",
    "    elif skew < -1: # If left skew floor, cap and exp(x)\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        exp(when(combined_df[col] < d[col][0], d[col][0]) \\\n",
    "        .when(indexed[col] > d[col][1], d[col][1]) \\\n",
    "        .otherwise(indexed[col])).alias(col))\n",
    "        print(col+ \"has been treated for negative (left) skewness. (skew = \",skew,\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_Score</th>\n",
       "      <th>A2_Score</th>\n",
       "      <th>A3_Score</th>\n",
       "      <th>A4_Score</th>\n",
       "      <th>A5_Score</th>\n",
       "      <th>A6_Score</th>\n",
       "      <th>A7_Score</th>\n",
       "      <th>A8_Score</th>\n",
       "      <th>A9_Score</th>\n",
       "      <th>A10_Score</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>age_num</th>\n",
       "      <th>gender_num</th>\n",
       "      <th>ethnicity_num</th>\n",
       "      <th>jundice_num</th>\n",
       "      <th>austim_num</th>\n",
       "      <th>contry_of_res_num</th>\n",
       "      <th>used_app_before_num</th>\n",
       "      <th>age_desc_num</th>\n",
       "      <th>relation_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1_Score  A2_Score  A3_Score  A4_Score  A5_Score  A6_Score  A7_Score  \\\n",
       "0         1         1         1         1         0         0         1   \n",
       "1         1         1         0         1         0         0         0   \n",
       "2         1         1         0         1         0         0         1   \n",
       "3         1         0         0         0         0         0         0   \n",
       "4         0         1         0         0         0         0         0   \n",
       "\n",
       "   A8_Score  A9_Score  A10_Score  ... label age_num gender_num ethnicity_num  \\\n",
       "0         1         0          0  ...   1.0    12.0        0.0           0.0   \n",
       "1         1         0          1  ...   1.0     6.0        1.0           7.0   \n",
       "2         1         0          1  ...   1.0    13.0        0.0           0.0   \n",
       "3         1         0          0  ...   1.0    19.0        0.0           2.0   \n",
       "4         1         0          0  ...   1.0    16.0        0.0           4.0   \n",
       "\n",
       "  jundice_num austim_num contry_of_res_num  used_app_before_num age_desc_num  \\\n",
       "0         0.0        0.0               0.0                  0.0          0.0   \n",
       "1         0.0        1.0               9.0                  0.0          0.0   \n",
       "2         0.0        1.0               0.0                  0.0          0.0   \n",
       "3         0.0        0.0              36.0                  0.0          0.0   \n",
       "4         0.0        0.0               0.0                  0.0          0.0   \n",
       "\n",
       "  relation_num  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          1.0  \n",
       "4          0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(indexed.count())\n",
    "indexed.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check for negative values in the dataframe. \n",
    "# Produce a warning if there are negative values in the dataframe that Naive Bayes cannot be used. \n",
    "# Note: we only need to check the numeric input values since anything that is indexed won't have negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|mins                          |\n",
      "+------------------------------+\n",
      "|[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "minimums = combined_df.select([min(c).alias(c) for c in combined_df.columns if c in numeric_inputs])\n",
    "min_array = minimums.select(array(numeric_inputs).alias(\"mins\"))\n",
    "min_array.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|max                            |\n",
      "+-------------------------------+\n",
      "|[1, 1, 1, 1, 1, 1, 1, 1, 1, 10]|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maximum = combined_df.select([max(c).alias(c) for c in combined_df.columns if c in numeric_inputs])\n",
    "max_array = maximum.select(array(numeric_inputs).alias(\"max\"))\n",
    "max_array.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+-----+\n",
      "|features                                                                 |label|\n",
      "+-------------------------------------------------------------------------+-----+\n",
      "|(19,[0,1,2,5,6,9,10],[1.0,1.0,1.0,1.0,1.0,6.0,12.0])                     |1.0  |\n",
      "|(19,[0,2,6,8,9,10,11,12,14,15],[1.0,1.0,1.0,1.0,5.0,6.0,1.0,7.0,1.0,9.0])|1.0  |\n",
      "|(19,[0,2,5,6,8,9,10,14],[1.0,1.0,1.0,1.0,1.0,6.0,13.0,1.0])              |1.0  |\n",
      "|(19,[6,9,10,12,15,18],[1.0,2.0,19.0,2.0,36.0,1.0])                       |1.0  |\n",
      "|(19,[0,6,9,10,12],[1.0,1.0,2.0,16.0,4.0])                                |1.0  |\n",
      "+-------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We need to Vectorize for df\n",
    "# Becasue the function that we use to make that correction requires a vector\n",
    "# create final feature list\n",
    "\n",
    "features_list = numeric_inputs + string_inputs\n",
    "# Create your vector assembler object\n",
    "assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "# And call on the vector assembler to transform your dataframe\n",
    "output = assembler.transform(indexed).select('features','label')\n",
    "output.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.000000, 1000.000000]\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                                       |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |[1000.0,1000.0,1000.0,0.0,0.0,1000.0,1000.0,0.0,0.0,600.0,260.8695652173913,0.0,0.0,0.0,0.0,0.0,0.0,500.0,0.0]                                 |\n",
      "|1.0  |[1000.0,0.0,1000.0,0.0,0.0,0.0,1000.0,0.0,1000.0,500.0,130.43478260869566,1000.0,636.3636363636364,0.0,1000.0,136.36363636363635,0.0,500.0,0.0]|\n",
      "|1.0  |[1000.0,0.0,1000.0,0.0,0.0,1000.0,1000.0,0.0,1000.0,600.0,282.6086956521739,0.0,0.0,0.0,1000.0,0.0,0.0,500.0,0.0]                              |\n",
      "|1.0  |[0.0,0.0,0.0,0.0,0.0,0.0,1000.0,0.0,0.0,200.0,413.04347826086956,0.0,181.8181818181818,0.0,0.0,545.4545454545454,0.0,500.0,200.0]              |\n",
      "|1.0  |[1000.0,0.0,0.0,0.0,0.0,0.0,1000.0,0.0,0.0,200.0,347.82608695652175,0.0,363.6363636363636,0.0,0.0,0.0,0.0,500.0,0.0]                           |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the mix max scaler object \n",
    "# This is what will correct for negative values\n",
    "# I like to use a high range like 1,000 \n",
    "#     because I only see one decimal place in the final_data.show() call\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",min=0,max=1000)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(output)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaled_data = scalerModel.transform(output)\n",
    "final_data = scaled_data.select('label','scaledFeatures')\n",
    "# Rename to default value\n",
    "final_data = final_data.withColumnRenamed(\"scaledFeatures\",\"features\")\n",
    "final_data.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = final_data.randomSplit([0.65, 0.35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                     |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1000.0,272.7272727272727,0.0,0.0,60.60606060606061,0.0,500.0,0.0]               |\n",
      "|1.0  |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,108.69565217391305,0.0,181.8181818181818,0.0,0.0,212.12121212121212,0.0,500.0,200.0]|\n",
      "|1.0  |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,108.69565217391305,0.0,363.6363636363636,0.0,0.0,60.60606060606061,0.0,500.0,0.0]   |\n",
      "|1.0  |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,173.91304347826087,1000.0,0.0,0.0,0.0,15.151515151515152,0.0,500.0,0.0]             |\n",
      "|1.0  |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,217.3913043478261,1000.0,90.9090909090909,0.0,0.0,45.45454545454545,0.0,500.0,0.0]  |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                        |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,478.26086956521743,0.0,545.4545454545454,0.0,0.0,45.45454545454545,0.0,500.0,400.0]    |\n",
      "|1.0  |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,500.0,1000.0,90.9090909090909,0.0,0.0,45.45454545454545,0.0,500.0,400.0]               |\n",
      "|1.0  |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,956.5217391304349,1000.0,181.8181818181818,0.0,0.0,363.6363636363636,0.0,500.0,200.0]  |\n",
      "|1.0  |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,100.0,0.0,0.0,363.6363636363636,0.0,0.0,303.03030303030306,0.0,500.0,0.0]                  |\n",
      "|1.0  |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,100.0,21.73913043478261,1000.0,181.8181818181818,0.0,0.0,75.75757575757576,0.0,500.0,200.0]|\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5, False)\n",
    "test.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import * \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bin_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction')\n",
    "MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Logistic Regression -without cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9973262032085561\n",
      "Accuracy: 99.73 %\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# This is the simplistic approach which does not use cross validation\n",
    "classifier = LogisticRegression()\n",
    "fitModel = classifier.fit(train)\n",
    "\n",
    "# Evaluation method for binary classification problem\n",
    "predictionAndLabels = fitModel.transform(test)\n",
    "auc = Bin_evaluator.evaluate(predictionAndLabels)\n",
    "print(\"AUC: \", auc)\n",
    "\n",
    "# Evaluation for a multiclass classification problem\n",
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_evaluator.evaluate(predictionAndLabels)*100)\n",
    "print(\"Accuracy: {0:.2f}\".format(accuracy),\"%\") #     print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression- with cross validation\n",
    "Spark has a build in validation functions to conduct cross validation which begins by splitting the training dataset into a set of \"flolds\" which are used as separate traning and test datasets. For example, with k=5 folds, CrossValidation will generate 5 different (training and testing) datasets pairs, each of which uses 4/5 of the data for traning and 1/5 for testing. To evaluate a particular Paramete, CrossValidation, computes the average evaluation metric for the 5 models produced by fitting the estimator on the 5 different (traning, tesing) dataset pairs and tells you which model performed the best once it funished. \n",
    "\n",
    "After identifying the best ParamMap, CrossValidation finally re-fits the Estimator using the best ParamMap and the entire dataset. \n",
    "\n",
    "***MaxIter***: The maximum number of iterations to use. There is no clear formula for setting the optimum iteration number, but you can figure out this issue by an iterative process by initializing the iteration number by a small number like 100 and then increase its lineary. This process will be repeated until the MSE of the test does not decrease and even may increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [77.06701654156629]\n",
      "Coefficients: \n",
      "DenseMatrix([[-9.63869022e-03, -7.84159335e-03, -9.66705685e-03,\n",
      "              -1.15161002e-02, -8.02302876e-03, -1.08283439e-02,\n",
      "              -9.78533030e-03, -1.18500117e-02, -7.35044193e-03,\n",
      "              -3.76266431e-02,  3.42061962e-03,  3.88274800e-04,\n",
      "               2.51851148e-03,  4.73925210e-05, -2.76285482e-04,\n",
      "               5.75394673e-03, -1.50626912e-02,  0.00000000e+00,\n",
      "               4.42420868e-03]])\n",
      "99.73190348525469\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "\n",
    "# Set up your parameter grid for the cross validator to conduct hyperparameter turning\n",
    "paramGrid = (ParamGridBuilder().addGrid(classifier.maxIter, [10, 15, 20]).build())\n",
    "\n",
    "# Set up the Cross Validation which requires all the following parameters: \n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MC_evaluator,\n",
    "                          numFolds=3)\n",
    "# Then fit the model\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Collect the best model \n",
    "# print the coefficient matrix\n",
    "# These values should be compared relative to eachother\n",
    "# And intercepts can be prepared to other models\n",
    "BestModel = fitModel.bestModel\n",
    "print(\"Intercept: \" + str(BestModel.interceptVector))\n",
    "print(\"Coefficients: \\n\" + str(BestModel.coefficientMatrix))\n",
    "\n",
    "# You can extract the best model from this run like this if you want\n",
    "LR_BestModel = BestModel\n",
    "\n",
    "# Next you need to generate predictions on the test dataset\n",
    "# fitModel automatically uses the best model\n",
    "# so we don't need to use BestModel here \n",
    "predictions = BestModel.transform(test)\n",
    "\n",
    "# Now print the accuracy rate of the model or AUC for a binary Classifier\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|        feature|               coeff|\n",
      "+---------------+--------------------+\n",
      "|       A2_Score|-0.00963869021536...|\n",
      "|       A3_Score|-0.00784159335336...|\n",
      "|       A4_Score|-0.00966705684667...|\n",
      "|       A5_Score|-0.01151610023448...|\n",
      "|       A6_Score|-0.00802302875616...|\n",
      "|       A7_Score|-0.01082834392296...|\n",
      "|       A8_Score|-0.00978533029837...|\n",
      "|       A9_Score|-0.01185001168206...|\n",
      "|      A10_Score|-0.00735044193473033|\n",
      "|            age|-0.03762664308840...|\n",
      "|         gender|0.003420619615851887|\n",
      "|      ethnicity|3.882747997514347...|\n",
      "|        jundice|0.002518511480284284|\n",
      "|         austim|4.739252099828245E-5|\n",
      "|  contry_of_res|-2.76285482482491...|\n",
      "|used_app_before|0.005753946727488514|\n",
      "|         result|-0.01506269120060...|\n",
      "|       age_desc|                 0.0|\n",
      "|       relation|0.004424208676235...|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create feature importance scores\n",
    "coeff_array = BestModel.coefficientMatrix.toArray()\n",
    "coeff_scores = []\n",
    "for x in coeff_array[0]:\n",
    "    coeff_scores.append(float(x))\n",
    "    \n",
    "result = spark.createDataFrame(zip(input_columns, coeff_scores), schema=['feature', 'coeff'])\n",
    "result.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A1_Score: integer (nullable = true)\n",
      " |-- A2_Score: integer (nullable = true)\n",
      " |-- A3_Score: integer (nullable = true)\n",
      " |-- A4_Score: integer (nullable = true)\n",
      " |-- A5_Score: integer (nullable = true)\n",
      " |-- A6_Score: integer (nullable = true)\n",
      " |-- A7_Score: integer (nullable = true)\n",
      " |-- A8_Score: integer (nullable = true)\n",
      " |-- A9_Score: integer (nullable = true)\n",
      " |-- A10_Score: integer (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ethnicity: string (nullable = true)\n",
      " |-- jundice: string (nullable = true)\n",
      " |-- austim: string (nullable = true)\n",
      " |-- contry_of_res: string (nullable = true)\n",
      " |-- used_app_before: string (nullable = true)\n",
      " |-- result: integer (nullable = true)\n",
      " |-- age_desc: string (nullable = true)\n",
      " |-- relation: string (nullable = true)\n",
      " |-- Class/ASD: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs. Rest\n",
    "The One-vs-Rest classifier is a type of multiclass classifier that involves training a single classifier per class, with the samples of the class as postive samples as negatives. So each is viewed as it compares to rest of the classes as a whole, as opposed to each one individaully. \n",
    "\n",
    "***regPram***: The purpose of the regularizer is to encourage simple models and avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mIntercept: \u001b[0m -11.983327410988968 \u001b[1m\n",
      "Coefficients:\u001b[0m [0.0014147237297437861,0.0011603159820726785,0.0014671375999953445,0.001993364161714782,0.0012841984039581094,0.0014588832285791563,0.0015841600567746382,0.0019684388933657935,0.001186584771856971,0.0062777716536192825,-0.00014706833064013616,-0.0001875245042105071,0.0003382119275422131,0.0004714857197136602,-1.6052576701380337e-05,-0.00042408314353121785,0.0016945819912884679,0.0,-0.0011546825535908238]\n",
      "\u001b[1mIntercept: \u001b[0m 11.983327410988933 \u001b[1m\n",
      "Coefficients:\u001b[0m [-0.0014147237297437861,-0.0011603159820726765,-0.0014671375999953408,-0.00199336416171478,-0.001284198403958107,-0.0014588832285791554,-0.0015841600567746395,-0.0019684388933657922,-0.0011865847718569669,-0.006277771653619247,0.00014706833064013865,0.00018752450421050847,-0.0003382119275422104,-0.0004714857197136609,1.6052576701379984e-05,0.00042408314353122094,-0.0016945819912884646,0.0,0.0011546825535908235]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.19571045576407"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the base Classifier\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Instantiate the One vs Rest Classifier\n",
    "classifier = OneVsRest(classifier=lr)\n",
    "\n",
    "# Add parameters of your choice here: \n",
    "paramGrid = ParamGridBuilder() \\\n",
    "            .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "            .build()\n",
    "\n",
    "# Cross Validator requires the following parameters:\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=2)\n",
    "\n",
    "# Cross Valiator requires the following prameters:\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Print the Coefficients\n",
    "# First we need to extract the best model from fit model\n",
    "\n",
    "# Get Best Model\n",
    "BestModel = fitModel.bestModel\n",
    "\n",
    "# Extract list of binary models\n",
    "models = BestModel.models\n",
    "\n",
    "for model in models:\n",
    "    print('\\033[1m' + 'Intercept: '+ '\\033[0m',model.intercept,'\\033[1m' + '\\nCoefficients:'+ '\\033[0m',model.coefficients)\n",
    "\n",
    "# Now generate predictions on test dataset\n",
    "predictions = BestModel.transform(test)\n",
    "\n",
    "# And Calculate the accuracy score\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Classifier\n",
    "#### Neural Network\n",
    "A multilayer perceptron (MLP) is a class of feedforward artifical neural network. It consist of at least three layers of nodes; an input layer, a hidden layer and an output layer. Expect for the input nodes, each nodes is a neuron that uses a nonlinear activation function. MLP utilizies a supervised learning technique called backpropagation for traning. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not lineraly separable.\n",
    "\n",
    "##### Common Hyper Parameters\n",
    "\n",
    "***MaxIter***: The maximum number of iterations to use. There is no clear formula for setting the optimum iteration number, but you can figure out this issue by an iterative process by initalizing the iteration number by a small number like 100 and then increase it linearly. This process will be replaced unitl the MSE of the test does not decrease and even may increase.\n",
    "\n",
    "***Layers***: Spark requires that the input layer equals the number of features in the dataset, the hidden layer might be one or two more that the (flexible), and the output layer has to be equal to the number of classes.\n",
    "\n",
    "***Block Size***: Block size for stacking input data in matrices to speed up the compuatation. Data is stacked within partitions. If block size is more than remaining data in a partition then it is abjusted to the size of this data. Recommended size is between 10 and 1000. Default: 128\n",
    "\n",
    "***Seed***: A random seed. Set the value if you need your results to be reproducible across repeated calls (highly recommended). \n",
    "\n",
    "***Weights***: Each hidden neuron added will increase the number of weights, thus it is recommended to use the least number of hidden neurons that accomplish the task. Using more hidden neurons that required will add more complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mModel Weights: \u001b[0m 839\n",
      "Accuracy:  88.73994638069705\n"
     ]
    }
   ],
   "source": [
    "# Count Features\n",
    "features = final_data.select(['features']).collect()\n",
    "features_count = len(features[0][0])\n",
    "\n",
    "# Count Classes\n",
    "class_count = final_data.select(countDistinct(\"label\")).collect()\n",
    "classes = class_count[0][0]\n",
    "\n",
    "# Then use this number to specify the layers\n",
    "# The first number in this list is the input layer which has to be equal to the number of features in your vector\n",
    "# The second number is the first hidden layer\n",
    "# The third number is the second hidden layer\n",
    "# The fourth number is the output layer which has to be equal to your class size\n",
    "layers = [features_count, features_count+1, features_count, classes]\n",
    "\n",
    "# Instaniate the classifier\n",
    "classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# Fit the model\n",
    "fitModel = classifier.fit(train)\n",
    "\n",
    "# Print the model weights\n",
    "print('\\033[1m' + \"Model Weights: \"+ '\\033[0m',fitModel.weights.size)\n",
    "\n",
    "# Generate predictions on test dataframe\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "\n",
    "# Print accuracy score\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navie Bayes\n",
    "The Navie Bayes Classifier is a collection of classification algorithms based on Bayes Theorem. It is not a single algrothim but a family of algrothims that all share a common principle, that every feature being classified is independent of the value of any other features.\n",
    "\n",
    "***Assumptions:***\n",
    "- Independence between every pair of features\n",
    "- Feature values are non-negative (which is why we checked earlier)\n",
    "\n",
    "***Hyper Parameters***:\n",
    "\n",
    "***Smoothing***: It is problematic when a frequency-based probaility is zero, because it will wipe out all the inforamtion in the other probailities, and we need to find a solution for this. A solution would be laplace smoothing, which is a techniques for smoothing categorical data. In PySpark, this number need to be >= 0, default is 1.0\n",
    "\n",
    "***Thresholds***:\n",
    "Thresholds is multi-class classification to adjust the probaility of predicting each class. Array must be length equal to the number of classes, with value >0, expecting that at most one value may be 0. The class with largest value p/t predicted, where p is the orignial probability of that class and t is the class's threshold. The default value os none.\n",
    "\n",
    "***weightCol***: \n",
    "If you have a weight column you would enter the name of the column here. If this is not set or empty, we treat all instance weight as 1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  85.25469168900804\n"
     ]
    }
   ],
   "source": [
    "# Add parameters of your choice here: \n",
    "classifier = NaiveBayes()\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "            .addGrid(classifier.smoothing, [0.0, 0.2, 0.4, 0.6]) \\\n",
    "            .build())\n",
    "\n",
    "# Cross Validation requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=2)\n",
    "\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine\n",
    "Linear SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes, which is why you can only use it for binary classification. Support vectors are the data points neareast to the hyperplane, the points of a data set that, if removed, would alter the postition of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set. Intuitively, th e further for the hyperplane our data points lie, the more confident we are that they have been correctly classified. We therefore want our data points to be as far away from the hyperplane as possbile, while still being on the correct side of it. So when new testing data is added, whatever side of the hyperplane it lands will decide the class that we assign to it.\n",
    "\n",
    "***Interpretting the Coefficients***: \n",
    "Each coefficients directions gives us the predicted class, so if you take the dot product of any point with the vector, you can tell on which side it is; if the dot product is postive, it belongs to the positive class, if it is negative it belongs to the neagative class. \n",
    "\n",
    "You can even learn something about the importance of each features. Let's say the svm would find only one feature useful for sparating data, then the hyperplane would be orthogonal to that axis. So, you could say that hte absolute size of the coefficient realtive to the other ones gives an indication of how important the feature was for the separation. \n",
    "\n",
    "***Hyper Parameters:***\n",
    "\n",
    "***MaxIter***: The maximum number of iterations to use. There is no clear formula for setting the opitmum iteration number, but you can figure out this issue by an iterative process by intializing the iteration number by a small number like 100 and then increase it linearly. This process will be repeated until the MSE of the test does no decrease and even may increas\n",
    "\n",
    "***regParam***: The purpose of the reqularizer is to encourage simple models and avoid overfitting. To learn more about this concept \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many classes you have and produce an error if it's more than 2\n",
    "class_count = final_data.select(countDistinct(\"label\")).collect()\n",
    "classes = class_count[0][0]\n",
    "\n",
    "if classes > 2:\n",
    "    print(\"LinearSVC cannot be used because PySpark currently only accepts binary classification data for this algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: \n",
      "10.50772938195986\n",
      "\u001b[1m Coefficients\u001b[0m\n",
      "You should compares these relative to eachother\n",
      "Coefficients: \n",
      "[-0.0014685941084461728,-0.0013007750487953693,-0.0015816498767680749,-0.0018233485768019743,-0.001322112807286529,-0.0013512235654348371,-0.0016221482480992463,-0.0018840637506359695,-0.0013300913893052723,-0.0033447293685842073,-0.00019721272412356674,0.00031061019558149594,-0.0005508875210642046,-0.000442275870867221,1.9417473241708744e-05,0.00010872281223349515,-0.00047003491317712195,0.0,0.0012189862022170446]\n",
      "Accuracy: 85.25469168900804\n"
     ]
    }
   ],
   "source": [
    "# Add Parameters of your choice here:\n",
    "classifer = LinearSVC()\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "             .build())\n",
    "\n",
    "# Cross Validation requires all of the following parameters: \n",
    "crossval = CrossValidator(estimator=classifer,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=MulticlassClassificationEvaluator(),\n",
    "                           numFolds=3)\n",
    "\n",
    "fitModel = crossval.fit(train)\n",
    "BestModel = fitModel.bestModel\n",
    "\n",
    "print(\"Intercept: \\n\" + str(BestModel.intercept))\n",
    "print('\\033[1m' + \" Coefficients\"+ '\\033[0m')\n",
    "print(\"You should compares these relative to eachother\")\n",
    "print(\"Coefficients: \\n\" + str(BestModel.coefficients))\n",
    "\n",
    "# Automatically gets the best model\n",
    "prdictions = BestModel.transform(test)\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Decision Trees classifier are a suprevised learning method is used to classify a variable by learning from historical data that the model uses to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the filtter the model. \n",
    "\n",
    "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subset while at the same time an associated decision tree which corresponds to the best predictor called root node. Decision trees can handle both categotical and numberical data. \n",
    "\n",
    "#### Common Hyper Parameters\n",
    "- ***maxBins*** = Max number of bins for discretizing continuous feature. Must be >=2 and <=  number of categories for any categorical feture.\n",
    "    - ***Continuous features***: For small datasets in single-machine implementaions, the split candidates for each continuous feature are typically the uunique values for the feature. Some implementations sort the feature values and then use the ordered unique value as split candidates for faster tree calculations. Sorting feature values is expensive for large distributed datasets. This implementation computers an approximate set of split candidates by performance a quantilte calculation over a sampled fraction of the data. The ordered spits create \"bins\" and the maximum number of such bins can be specified using the MaxBins parameters. Note that the number of bins cannot be greater than the number of instances N (a rare scenario since the default maxBins value in 32). The tree algrothims automatically reduces the number of bins if the condition is not satisfied\n",
    "    \n",
    "    - ***Categorical features***: For a categorical feature with M possible values (categories), one could come up with 2 exp(M-1) split candidates. For binary(0/1) classification and regression, we can reduce the number of split candidates to M-1 by ordering the categorical feature values by the average label. For example, for a binary classification problem with one categorial features with three categories A,B and C whose corresponding proportions of label 1 are 0.2, 0.6, and 0.4, the categorical features are ordered as A, C, B. The two split candidates are A|C, B and A,C|B where denotes the split. In multiclass classification, all 2 exp(M-1)-1 possible splits are used whenever possbile. When 2 exp(M-1)-1 is geater than the maxBins parameters, we use a (heuristic) method similar to the method used for binary classifiaction and regression. The M categorical feature values are ordered by impurity, and the resulting M-1 split candidates are considered.\n",
    "    \n",
    "- ***MaxDepth*** = The max_depth parameter specifies the maximium depth of each tree. The deafult value of max_depth is None, which means that each tree will expand until evey leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class.\n",
    "\n",
    "#### Feature Importance Scores\n",
    "Scores add up to 1 across all variables so the lowest socre is the least importance varaible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Accuracy;  85.25469168900804\n"
     ]
    }
   ],
   "source": [
    "# Add parameters of your choice here:\n",
    "classifier = DecisionTreeClassifier()\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "            .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "            .build())\n",
    "\n",
    "# Cross Validator requires all of the folllowing parameters: \n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid, \n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds = 2)\n",
    "\n",
    "# Fit model: Run cross-validation, and choose the best set of parameters\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Collect and print feature importances\n",
    "\n",
    "BestModel = fitModel.bestModel\n",
    "featureImportances = BestModel.featureImportances.toArray()\n",
    "print(\"Feature Importances: \", featureImportances)\n",
    "\n",
    "predictios = fitModel.transform(test)\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "print(\"Accuracy; \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_487f876902476a872a89) of depth 1 with 3 nodes\n",
      "  If (feature 9 <= 650.0)\n",
      "   Predict: 1.0\n",
      "  Else (feature 9 > 650.0)\n",
      "   Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(BestModel.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|feature        |score|\n",
      "+---------------+-----+\n",
      "|age            |1    |\n",
      "|contry_of_res  |0    |\n",
      "|used_app_before|0    |\n",
      "|A9_Score       |0    |\n",
      "|result         |0    |\n",
      "|A10_Score      |0    |\n",
      "|A7_Score       |0    |\n",
      "|A5_Score       |0    |\n",
      "|ethnicity      |0    |\n",
      "|jundice        |0    |\n",
      "|austim         |0    |\n",
      "|gender         |0    |\n",
      "|A8_Score       |0    |\n",
      "|age_desc       |0    |\n",
      "|relation       |0    |\n",
      "|A6_Score       |0    |\n",
      "|A4_Score       |0    |\n",
      "|A2_Score       |0    |\n",
      "|A3_Score       |0    |\n",
      "+---------------+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Zip input_columns qith feature importance scores and create df\n",
    "# First convert feaureimportance scores from numpy array to list\n",
    "imp_scores = []\n",
    "for x in featureImportances:\n",
    "    imp_scores.append(int(x))\n",
    "    \n",
    "# Then zip with input_columns list and create a df\n",
    "result = spark.createDataFrame(zip(input_columns,imp_scores), schema=['feature','score'])\n",
    "print(result.orderBy(result[\"score\"].desc()).show(truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Suppose you have a training set with 6 classes, random forest may create three decision trees taking input of each subset. Finally, it predicts based on the majority of votes from each of the decision trees made. This works well becasue a single decision tree may be prone to noise, but aggregate of many decision trees reduce the effect of noise giving more accurate results. The subsets in different decision trees created may overlap. \n",
    "\n",
    "##### Common Hyper Parameter\n",
    "- **maxBins** = Max number of bins for discretizing continuous features. Must be >= 2 and >= number of categories for any catgorical features. \n",
    "    - ***Continuous features***: For small datasets in single-machine implementations, the split candidates for each continuous features are typically the unique values for the feature. Some implementations sort the feature values and then use the ordered unique values as split candidates for faster tree calculations. Sorting features values is expenisive for large distribuited datasets. This implementation computes an approximate set of split candidates by peroformaing a quantile calculation over a sampled fraction of the data. The ordered splits create \"bins\" and the maxumum number of such bins can be specified using the maxBins parmeters. Note that the number of bins cannot be greater than the number of instance N. The tree algrothim automatically reduces the number of bins if the condition is not satisfied.\n",
    "    - ***Categorical features***: For a categorical feature with M possible values (categories), one could come up with 2 exp(M-1)-1 split candidates. For binary (0/1) classification and regression, we can reduce the number of split candidates to M-1 by ordering the categroical feature values by the average label. For exampe, for a features are ordered as, A, C, B. The two split candidates are A| C, B and A, C | B where | denodes the split.\n",
    "        - In multiclass classification, all 2 exp(M-1)-1 possible splits are used whenever possible. When 2 exp(M-1)-1 is greater than the maxBins parameter, we use a method similar to the method used for binary classification and regression. The M categorical feature values are ordered by impurity, and the resulting M-1 split candidates are considered. \n",
    "- **maxDepth** = The maxDepth parameter specifies the maximum depth of each tree. The default value for max_depth is None, which means that each tree will expand until every leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class. \n",
    "\n",
    "#### Feature Importance Scores\n",
    "Scores add up to 1 accross all variables so the lowest score is the least importance variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " \n",
      "Accuracy:  85.25469168900804\n"
     ]
    }
   ],
   "source": [
    "# Add parameters of your choice here\"\n",
    "classifier = RandomForestClassifier()\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "             .addGrid(classifier.maxDepth, [2, 5, 10])\n",
    "                                 .addGrid(classifier.maxBins, [5, 10, 20])\n",
    "                                 .addGrid(classifier.numTrees, [5, 20, 50])\n",
    "                     .build())\n",
    "\n",
    "# Cross Validator requires all of the following parameters: \n",
    "crossval = CrossValidator(estimator = classifier, \n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds = 2)\n",
    "\n",
    "# Fit Model: Run Cross-Validation, and choose the best set of parameters. \n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "# Retrieve best model from cross val\n",
    "BestModel = fitModel.bestModel\n",
    "featureImportance = BestModel.featureImportances.toArray()\n",
    "print(\"Feature Importance: \", featureImportances)\n",
    "\n",
    "predictions = (MC_evaluator.evaluate(predictions))*100\n",
    "print(\" \")\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost Tree Classifer\n",
    "It's more of a hierarchical approch. It combines the weak learners to strong prediction rules that allow a flexble partition of the feature space. The objective here, as is of my supervised learning algorthim, is to define a loss function and minimize it. \n",
    "\n",
    "### Common Hyper Parameters\n",
    "\n",
    " - **maxBins** = Max number of bins for discretizing continuous features. Must be >=2 and >= number of categories for any categorical feature.\n",
    "     - **Continuous features:** For small datasets in single-machine implementations, the split candidates for each continuous feature are typically the unique values for the feature. Some implementations sort the feature values and then use the ordered unique values as split candidates for faster tree calculations.\n",
    "         Sorting feature values is expensive for large distributed datasets. This implementation computes an approximate set of split candidates by performing a quantile calculation over a sampled fraction of the data. The ordered splits create “bins” and the maximum number of such bins can be specified using the maxBins parameter.\n",
    "         Note that the number of bins cannot be greater than the number of instances N (a rare scenario since the default maxBins value is 32). The tree algorithm automatically reduces the number of bins if the condition is not satisfied.\n",
    "\n",
    "     - **Categorical features:** For a categorical feature with M possible values (categories), one could come up with 2 exp(M−1) −1 split candidates. For binary (0/1) classification and regression, we can reduce the number of split candidates to M−1 by ordering the categorical feature values by the average label. For example, for a binary classification problem with one categorical feature with three categories A, B and C whose corresponding proportions of label 1 are 0.2, 0.6 and 0.4, the categorical features are ordered as A, C, B. The two split candidates are A | C, B and A , C | B where | denotes the split.\n",
    "         In multiclass classification, all 2 exp(M−1) −1 possible splits are used whenever possible. When 2 exp(M−1) −1 is greater than the maxBins parameter, we use a (heuristic) method similar to the method used for binary classification and regression. The M categorical feature values are ordered by impurity, and the resulting M−1 split candidates are considered.\n",
    "         \n",
    " - **maxDepth** = The maxDepth parameter specifies the maximum depth of each tree. The default value for max_depth is None, which means that each tree will expand until every leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class.\n",
    "\n",
    "### Feature Importance Scores\n",
    "Scores add up to 1 accross all varaibles so the lowest score is the least imporant variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:  [1.28640033e-02 1.90900505e-02 1.13649268e-02 1.45030832e-01\n",
      " 8.93425545e-02 2.10258751e-03 1.34570001e-02 6.36346063e-02\n",
      " 2.21144441e-02 6.04253296e-01 4.34643588e-03 9.66584345e-05\n",
      " 6.49135824e-03 0.00000000e+00 2.41598675e-04 4.42450528e-03\n",
      " 3.93715152e-04 0.00000000e+00 7.51427825e-04]\n",
      " \n",
      "Accuracy:  100.0\n"
     ]
    }
   ],
   "source": [
    "class_count = final_data.select(countDistinct(\"label\")).collect()\n",
    "classes = class_count[0][0]\n",
    "if classes > 2:\n",
    "    print(\"GBTClassifier cannot be used because PySpark currently only accepts binary classification data for this algorithm\")\n",
    "\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "#                              .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "#             .addGrid(classifier.maxIter, [10, 15,50,100])\n",
    "             .build())\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=2) # 3 + is best practice\n",
    "\n",
    "# Fit Model: Run cross-validation, and choose the best set of parameters.\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "BestModel = fitModel.bestModel\n",
    "featureImportances = BestModel.featureImportances.toArray()\n",
    "print(\"Feature Importances: \",featureImportances)\n",
    "    \n",
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "print(\" \")\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
